{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step-by-Step Example: Fine-Tuning BERT on Azure ML"
      ],
      "metadata": {
        "id": "ZoXsTGF5tD7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Step 1: Set Up Azure Machine Learning Workspace\n",
        "\n",
        "1. **Create an Azure ML Workspace**:\n",
        "   - Go to the [Azure Portal](https://ml.azure.com/).\n",
        "   - Create a new resource group if you don't have one.\n",
        "   - Search for \"Machine Learning\" and create a new Azure Machine Learning workspace.\n",
        "\n",
        "2. **Install Azure ML SDK**:\n",
        "   - Install the Azure ML SDK on your local machine or in an Azure ML Notebook."
      ],
      "metadata": {
        "id": "gX5P9lpQtJfz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saqzE58rs3a5"
      },
      "outputs": [],
      "source": [
        "pip install azureml-core azureml-sdk azureml-widgets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Configure the Workspace"
      ],
      "metadata": {
        "id": "AljygeogtXS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "# Connect to the workspace\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# Print workspace details\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
      ],
      "metadata": {
        "id": "v_LbLBERtYDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Prepare the Data\n",
        "\n",
        "1. **Upload Data to Azure Blob Storage**:\n",
        "   - Upload your dataset (e.g., CSV file) to Azure Blob Storage."
      ],
      "metadata": {
        "id": "WtqCkKa4tcf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Datastore, Dataset\n",
        "\n",
        "# Get the default datastore\n",
        "datastore = ws.get_default_datastore()\n",
        "\n",
        "# Upload the dataset\n",
        "datastore.upload(src_dir='data/', target_path='datasets/', overwrite=True)\n",
        "\n",
        "# Create a dataset\n",
        "dataset = Dataset.Tabular.from_delimited_files(path=(datastore, 'datasets/your_dataset.csv'))"
      ],
      "metadata": {
        "id": "JBOk0xNKtdY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Create a Compute Cluster"
      ],
      "metadata": {
        "id": "HP6tXU93tiaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Define the compute cluster\n",
        "compute_name = \"cpu-cluster\"\n",
        "compute_min_nodes = 0\n",
        "compute_max_nodes = 4\n",
        "vm_size = \"STANDARD_D2_V2\"\n",
        "\n",
        "# Create the compute cluster\n",
        "try:\n",
        "    compute_target = ComputeTarget(workspace=ws, name=compute_name)\n",
        "    print(\"Found existing compute target.\")\n",
        "except ComputeTargetException:\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
        "                                                           min_nodes=compute_min_nodes,\n",
        "                                                           max_nodes=compute_max_nodes)\n",
        "    compute_target = ComputeTarget.create(ws, compute_name, compute_config)\n",
        "\n",
        "compute_target.wait_for_completion(show_output=True)"
      ],
      "metadata": {
        "id": "d9ncIOmgtoOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 5: Define the Training Script"
      ],
      "metadata": {
        "id": "9uQbuapQtq5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train.py\n",
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments, BertTokenizer\n",
        "\n",
        "def main(args):\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(args.data_path)\n",
        "    texts = df['text'].tolist()\n",
        "    labels = df['label'].tolist()\n",
        "\n",
        "    # Tokenize data\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    encodings = tokenizer(texts, truncation=True, padding=True)\n",
        "\n",
        "    # Create dataset\n",
        "    class Dataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, encodings, labels):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "            return item\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "    dataset = Dataset(encodings, labels)\n",
        "\n",
        "    # Load model\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=64,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Save model\n",
        "    model.save_pretrained(args.output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data_path', type=str, help='Path to the training data')\n",
        "    parser.add_argument('--output_dir', type=str, help='Path to save the trained model')\n",
        "    args = parser.parse_args()\n",
        "    main(args)"
      ],
      "metadata": {
        "id": "ZETKGiGmtrkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 6: Create an Environment"
      ],
      "metadata": {
        "id": "NQPVD41Ktw6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "\n",
        "# Create an environment\n",
        "env = Environment.from_conda_specification(name='bert-env', file_path='environment.yml')\n",
        "\n",
        "# environment.yml\n",
        "name: bert-env\n",
        "channels:\n",
        "  - defaults\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - pip:\n",
        "    - transformers\n",
        "    - torch\n",
        "    - pandas\n",
        "    - scikit-learn"
      ],
      "metadata": {
        "id": "soqs0ViNtwmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 7: Submit the Training Job"
      ],
      "metadata": {
        "id": "r2Ufh2mWt2El"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import ScriptRunConfig, Experiment\n",
        "\n",
        "# Create a script run configuration\n",
        "src = ScriptRunConfig(source_directory='./scripts',\n",
        "                      script='train.py',\n",
        "                      arguments=['--data_path', dataset.as_named_input('input').as_mount(),\n",
        "                                 '--output_dir', './outputs'],\n",
        "                      compute_target=compute_target,\n",
        "                      environment=env)\n",
        "\n",
        "# Create an experiment\n",
        "experiment = Experiment(workspace=ws, name='bert-fine-tuning')\n",
        "\n",
        "# Submit the experiment\n",
        "run = experiment.submit(src)\n",
        "run.wait_for_completion(show_output=True)"
      ],
      "metadata": {
        "id": "WgFRUdVOt5YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 8: Deploy the Model"
      ],
      "metadata": {
        "id": "5iXCn-P5t7m8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.model import Model\n",
        "from azureml.core.webservice import AciWebservice, Webservice\n",
        "from azureml.core.model import InferenceConfig\n",
        "\n",
        "# Register the model\n",
        "model = Model.register(workspace=ws, model_name='bert-model', model_path='./outputs')\n",
        "\n",
        "# Define inference configuration\n",
        "inference_config = InferenceConfig(entry_script='score.py', environment=env)\n",
        "\n",
        "# Define deployment configuration\n",
        "aci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
        "\n",
        "# Deploy the model\n",
        "service = Model.deploy(workspace=ws,\n",
        "                       name='bert-service',\n",
        "                       models=[model],\n",
        "                       inference_config=inference_config,\n",
        "                       deployment_config=aci_config)\n",
        "service.wait_for_deployment(show_output=True)\n",
        "\n",
        "print(service.scoring_uri)"
      ],
      "metadata": {
        "id": "i4oiviMgt-Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 9: Create the Scoring Script\n",
        "\n",
        "Create a scoring script (`score.py`) for the deployed model."
      ],
      "metadata": {
        "id": "DcqI2H7Jt9w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# score.py\n",
        "import json\n",
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "def init():\n",
        "    global model\n",
        "    global tokenizer\n",
        "    model_path = Model.get_model_path('bert-model')\n",
        "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def run(data):\n",
        "    try:\n",
        "        inputs = json.loads(data)\n",
        "        texts = inputs['texts']\n",
        "        encodings = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\n",
        "        outputs = model(**encodings)\n",
        "        predictions = torch.argmax(outputs.logits, dim=1).tolist()\n",
        "        return json.dumps(predictions)\n",
        "    except Exception as e:\n",
        "        error = str(e)\n",
        "        return json.dumps({\"error\": error})"
      ],
      "metadata": {
        "id": "7evjMa22uC2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "This step-by-step example demonstrates how to fine-tune a BERT model for text classification using Azure Machine Learning. It covers setting up the environment, preparing the data, training the model, and deploying the model as a web service."
      ],
      "metadata": {
        "id": "czvt-oEyuJeK"
      }
    }
  ]
}